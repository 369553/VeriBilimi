{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdc051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pa\n",
    "import numpy as nm\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM, Input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99adf4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrain, yTrain), (xTest, yTest) = imdb.load_data()\n",
    "indexes = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2754f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n"
     ]
    }
   ],
   "source": [
    "print(len(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2154cb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rakamının indeksi: 2238\n",
      "'the' ilgisiz kelîmesinin indeksi: 1\n",
      "'a' ilgisiz kelîmesinin indeksi: 3\n",
      "'I' ilgisiz zamirinin indeksi: 126\n"
     ]
    }
   ],
   "source": [
    "# Duygu analizi için fazla kelîme var.\n",
    "# Önişlemede kelîmelerin süzgeçten geçirilip, geçirilmediğine\n",
    "# bakılabilir; bunun için birkaç deneme yapabiliriz:\n",
    "print(f\"0 rakamının indeksi: {indexes['0']}\")\n",
    "print(f\"'the' ilgisiz kelîmesinin indeksi: {indexes['the']}\")\n",
    "print(f\"'a' ilgisiz kelîmesinin indeksi: {indexes['a']}\")\n",
    "print(f\"'I' ilgisiz zamirinin indeksi: {indexes['your']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabba5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rakamının indeksi: 2238\n",
      "'the' ilgisiz kelîmesinin indeksi: 1\n",
      "'a' ilgisiz kelîmesinin indeksi: 3\n",
      "'I' ilgisiz zamirinin indeksi: 126\n"
     ]
    }
   ],
   "source": [
    "# Duygu analizinde genellikle kelîmelerin sırası önemli değildir.\n",
    "# Çünkü biz, ne yorumun anlamını anlamaya çalışıyor,\n",
    "# ne çeviri yapmaya çalışıyor; ne de bahsettiği konuyla ilgileniyoruz;\n",
    "# Biz sadece olumlu yorumlarda saptanan desenleri ve olumsuz yorumlarda\n",
    "# saptanan desenleri anlamaya çalışıyoruz.\n",
    "# Bu, genellikle kelîme frekansı gibi basit analizle çözülebilir.\n",
    "# Dikkat, kelîmelerin sırasının öneminin olmaması, kelîmelerin\n",
    "# yinelenen sinir ağında işlenmesine gerek olmadığı anlamına gelmez.\n",
    "# Çünkü, kelîmelerin az da olsa bağlam bilgisinin çıkarılması,\n",
    "# model performansı için önemlidir.\n",
    "\n",
    "# Bu proje için;\n",
    "# En çok kullanılan 15000 kelîme üzerinde duygu analizi\n",
    "# yapmaya çalışıp, yetersiz olunması durumunda veri seti üzerinde\n",
    "# temizlik yapmaya girişilebilir:\n",
    "\n",
    "# En çok kullanılan 15000 kelîmeyi almak istersek;\n",
    "(xTrain, yTrain), (xTest, yTest) = imdb.load_data(num_words = 15000)\n",
    "print(f\"0 rakamının indeksi: {indexes['0']}\")\n",
    "print(f\"'the' ilgisiz kelîmesinin indeksi: {indexes['the']}\")\n",
    "print(f\"'a' ilgisiz kelîmesinin indeksi: {indexes['a']}\")\n",
    "print(f\"'I' ilgisiz zamirinin indeksi: {indexes['your']}\")\n",
    "# Durma kelîmeleri hâlen duruyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc80d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelîmelerin eşit uzunluğa getirilmesi gerekiyor:\n",
    "maxLen = 100# Cümle uzunluğu\n",
    "xTrain = pad_sequences(xTrain, maxlen = maxLen)\n",
    "xTest = pad_sequences(xTest, maxlen = maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87b54c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,250,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">55,040</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │       \u001b[38;5;34m2,250,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m55,040\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │           \u001b[38;5;34m1,040\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,306,097</span> (8.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,306,097\u001b[0m (8.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,306,097</span> (8.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,306,097\u001b[0m (8.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Katmanların, model çağrılarının ve modelin (Sequential API ile) oluşturulması\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(maxLen,)))\n",
    "model.add(Embedding(\n",
    "    input_dim = 15000, output_dim = 150, input_length = maxLen\n",
    "    ))\n",
    "model.add(LSTM(64, activation = 'relu'))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "callEarlyStopping = EarlyStopping(monitor = 'val_loss', patience = 3,\n",
    "                                  restore_best_weights = True)\n",
    "\n",
    "filePath = \"C:\\AIMLProjects\\Keras\\imdbSentimentAnalysis\\model.h5\"\n",
    "callModelCheckpoint = ModelCheckpoint(filePath, save_best_only = True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41bcff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742ms/step - accuracy: 0.5936 - loss: 280.0533"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 824ms/step - accuracy: 0.5943 - loss: 284.4839 - val_accuracy: 0.6916 - val_loss: 24.3266\n",
      "Epoch 2/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 761ms/step - accuracy: 0.7106 - loss: 70862032.0000 - val_accuracy: 0.5840 - val_loss: 6220899.5000\n",
      "Epoch 3/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 710ms/step - accuracy: 0.5746 - loss: 895161.8125 - val_accuracy: 0.5914 - val_loss: 2794.2463\n",
      "Epoch 4/15\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 747ms/step - accuracy: 0.6585 - loss: 2637.3892 - val_accuracy: 0.6856 - val_loss: 1724.3610\n",
      "Doğruluk : 0.6909999847412109\n",
      "Kayıp : 2109.111572265625\n",
      "Doğrulama seti doğruluğu : 0.6855999827384949\n",
      "Doğrulama kaybı : 1724.3609619140625\n"
     ]
    }
   ],
   "source": [
    "# Modelin derlenmesi ve eğitim:\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x = xTrain, y = yTrain, epochs = 15, batch_size = 256,\n",
    "          validation_split = 0.2,\n",
    "          callbacks = [callEarlyStopping, callModelCheckpoint]\n",
    "          )\n",
    "\n",
    "\n",
    "res = model.history.history\n",
    "\n",
    "print(f\"Doğruluk : {res['accuracy'][-1]}\")# 0.8217499852180481\n",
    "print(f\"Kayıp : {res['loss'][-1]}\")# 0.4583764672279358\n",
    "print(f\"Doğrulama seti doğruluğu : {res['val_accuracy'][-1]}\")# 0.6930000185966492\n",
    "print(f\"Doğrulama kaybı : {res['val_loss'][-1]}\")# 0.5820649862289429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "926898cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 251ms/step - accuracy: 0.6932 - loss: 932790.0000\n",
      "Test kaybı: 3754645.5\n",
      "Test Doğruluğu: 0.6926000118255615\n"
     ]
    }
   ],
   "source": [
    "testLoss, testAccuracy = model.evaluate(xTest, yTest, batch_size = 256)\n",
    "\n",
    "print(f\"Test kaybı: {testLoss}\")# 0.5038241147994995\n",
    "print(f\"Test Doğruluğu: {testAccuracy}\")# 0.7570800185203552\n",
    "\n",
    "# Görüldüğüz üzere başarı %~70\n",
    "# ve böylesine basit bir görev için bu çok düşük!..\n",
    "\n",
    "# Sorun ne??\n",
    "# Sorun, durma kelîmelerinin sürekli geçmesinden ötürü\n",
    "# 88584 kelîme içerisinden en çok geçen 15 bin kelîmeyi aldığımızda\n",
    "# pek çok durma kelîmesini alıyor oluşumuzdur.\n",
    "# Durma kelîmeleri ('the', 'a', 'an' gibi) hem olumlu, hem de\n",
    "# olumsuz yorumlarda geçtiğinden model başarısı yükselmiyor\n",
    "# Ayrıca modelin aşırı uyumunu engellemek için hiç 'Dropout'\n",
    "# katmanı eklemedik; bu kadar fazla parametreye 'Dropout' eklenmeden olmaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a1305d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yeni verilerle çalışmadan evvel hâfızayı temizlemek münâsip olur:\n",
    "del xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11e98125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Çözüm önerisi:\n",
    "\n",
    "# Her kelîmenin indeksini bildiğimize göre kelîmeleri\n",
    "# metîn olarak alıp, üzerinde ön işleme yapabiliriz:\n",
    "\n",
    "# Her sayının yerine kelîmeyi koymak yerine\n",
    "# Veri setinin ham hâlini indirmek daha uygun olur.\n",
    "# Veri setinin asıl adresi şudur:\n",
    "#r\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "# Fakat arşivden çıkarma gibi faaliyetlerden sıyrılmak için\n",
    "# pandas ile verilen adresten verinin çekildiği fonksiyonu kullanabiliriz\n",
    "# Bu verisi HuggingFace ile sunuluyor:\n",
    "# r\"hf://datasets/scikit-learn/imdb/IMDB Dataset.csv\"\n",
    "# Bunun için huggingface_hub ve fsspec kütüphânelerini kurmak lazım:\n",
    "# pip install huggingface_hub, fsspec\n",
    "# Daha fazla bilgi ve atıf burada mevcut: https://huggingface.co/datasets/scikit-learn/imdb\n",
    "dataURL = r\"hf://datasets/scikit-learn/imdb/IMDB Dataset.csv\"\n",
    "\n",
    "import pandas as pa\n",
    "dfData = pa.read_csv(dataURL)\n",
    "dfOriginal = dfData.copy()\n",
    "# İlk yorumlara bakmak istersek:\n",
    "#                                               review sentiment\n",
    "# 0  One of the other reviewers has mentioned that ...  positive\n",
    "# 1  A wonderful little production. <br /><br />The...  positive\n",
    "# 2  I thought this was a wonderful way to spend ti...  positive\n",
    "# 3  Basically there's a family where a little boy ...  negative\n",
    "# 4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
    "\n",
    "# Veri ciddî şekilde bozuk!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4bdfa208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri ön işleme ve temizlik (durma kelîmelerin dışındakiler):\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Şu uyarıları bir kapatmak lazım:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tüm kelîmeleri küçük harfe çevir:\n",
    "dfData[\"review\"] = [i.lower() for i in dfData[\"review\"]]\n",
    "\n",
    "# Bağlantı adreslerini çıkart:\n",
    "patternURL = r\"(http[s]?://\\S+)|(www.\\S+)\"\n",
    "dfData['review'].replace(patternURL, '', regex = True, inplace = True)\n",
    "\n",
    "# E - posta adreslerinin çıkartılması:\n",
    "patternEmail = r\"\\S+@\\S+\"\n",
    "dfData['review'].replace(patternEmail, '', regex = True, inplace = True)\n",
    "\n",
    "# Târih ve saatlerin çıkartılması: 1st.05.2024, 01.05.2024, 1.5.2024, 1.5.24\n",
    "# bunların diğer kombinasyonları ve # nokta (.) yerine '-' ile kullanılan versiyonları\n",
    "patternDate = r\"\\d{1,2}(th|st|rd|nd)?[\\.-/]\\d{1,2}[\\.-/]\\d{2,4}\"\n",
    "dfData['review'].replace(patternDate, '', regex = True, inplace = True)\n",
    "\n",
    "# HTML ve XML gibi etiket betik dillerinin kodlarının çıkartılması:\n",
    "# Ayrac işâretinin varlığına ve mevkîsine göre üç çeşit var : <br/> <html> </html>\n",
    "# Fakat veri setinde arada boşluk olduğundan <br /> ifâdesi düzenli ifâde kapsamında olmalı:\n",
    "patternScript = r\"<[/]?\\S+[ ]?[/]?>\"\n",
    "dfData['review'].replace(patternScript, '', regex = True, inplace = True)\n",
    "\n",
    "# Noktalama işâretlerinin çıkartılması:\n",
    "# puncs = \"[!\\\"#$%&'()*+,-./:;<=>?@^_{|}~9}]`\"\n",
    "dfData['review'] = [i.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for i in dfData['review']]\n",
    "\n",
    "# Metnin kelîmelere ayrılması ve boşlukların kaldırılması:\n",
    "dfData['review'] = [i.split() for i in dfData['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6107cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yazılım\n",
      "[nltk_data]     alanı\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk cümlenin uzunluğu (kelîme sayısı olarak): 314\n",
      "İkinci cümlenin uzunluğu (kelîme sayısı olarak): 160\n",
      "Temizlenmiş ilk cümlenin uzunluğu (kelîme sayısı olarak): 163\n",
      "Temizlenmiş ikinci cümlenin uzunluğu (kelîme sayısı olarak): 86\n"
     ]
    }
   ],
   "source": [
    "# Veri ön işleme, devâmı (durma kelîmelerinin çıkarılması)\n",
    "# Durma kelîmelerinin çıkartılması:\n",
    "import nltk# Eğer yüklemediyseniz 'pip install nltk' kodu ile yükleyiniz.\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Durma kelîmelerini çıkarmadan evvel, iki metnin uzunluğuna bakmak istersek;\n",
    "# Dikkat, bu işlem uzun sürmektedir; bu işlem yerine buradaki her bir\n",
    "# kelîmeyi aralarına veyâ (|) işâreti koyarak bir düzenli ifâde hâline\n",
    "# getirmek performansı arttırabilir.\n",
    "firstSentenceLen = len(dfData['review'][0])\n",
    "secondSentenceLen = len(dfData['review'][1])\n",
    "print(f\"İlk cümlenin uzunluğu (kelîme sayısı olarak): {firstSentenceLen}\")\n",
    "print(f\"İkinci cümlenin uzunluğu (kelîme sayısı olarak): {secondSentenceLen}\")\n",
    "# Temizlenmiş ilk cümlenin uzunluğu (kelîme sayısı olarak): 307\n",
    "# Temizlenmiş ikinci cümlenin uzunluğu (kelîme sayısı olarak): 162\n",
    "dfData['review'] = [[i for i in sent if i not in stopwords.words('english')] for sent in dfData['review']]\n",
    "\n",
    "firstSentenceLen = len(dfData['review'][0])\n",
    "secondSentenceLen = len(dfData['review'][1])\n",
    "print(f\"Temizlenmiş ilk cümlenin uzunluğu (kelîme sayısı olarak): {firstSentenceLen}\")\n",
    "print(f\"Temizlenmiş ikinci cümlenin uzunluğu (kelîme sayısı olarak): {secondSentenceLen}\")\n",
    "# Temizlenmiş ilk cümlenin uzunluğu (kelîme sayısı olarak): 171\n",
    "# Temizlenmiş ikinci cümlenin uzunluğu (kelîme sayısı olarak): 90\n",
    "\n",
    "# Görüldüğü üzere ilk cümlenin neredeyse yarısı anlamsız kelîmelerden\n",
    "# oluşuyormuş; ikinci cümlede ise 72 adet anlamsız kelîmeden kurtulmuş olduk\n",
    "# Bu büyük veri temizliğinden sonra kelîmelerin köklerini bulmaya başlanabilir.\n",
    "\n",
    "# Veriyi kaybetmemek için kaydedebilirsiniz:\n",
    "dfData.to_json('cleaned_IMDB.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b6f9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Görüldüğü üzere ilk cümlenin neredeyse yarısı anlamsız kelîmelerden\n",
    "# oluşuyormuş; ikinci cümlede ise 72 adet anlamsız kelîmeden kurtulmuş olduk\n",
    "# Bu büyük veri temizliğinden sonra kelîmelerin köklerini bulma işlemine başlanabilir.\n",
    "\n",
    "# Kelîme gövdelerinin elde edilmesi:\n",
    "# Kelîme gövdelerinin elde edilmesi için yardımcı kütüphâneye ihtiyaç var\n",
    "# nltk kütüphânesi bu işlem için destek veriyor..\n",
    "from nltk.stem import PorterStemmer\n",
    "prStem = PorterStemmer()\n",
    "dfData['review'] = [[prStem.stem(word) for word in sentence] for sentence in dfData['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4538af0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asıl veri:\n",
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
      "\n",
      "Temizlenmiş yorum:\n",
      "one review mention watch 1 oz episod hook right exactli happen first thing struck oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use word call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani aryan muslim gangsta latino christian italian irish scuffl death stare dodgi deal shadi agreement never far away would say main appeal show due fact goe show dare forget pretti pictur paint mainstream audienc forget charm forget romanc oz mess around first episod ever saw struck nasti surreal say readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard sold nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort view that get touch darker side\n"
     ]
    }
   ],
   "source": [
    "# İlk örnek üzerinden verinin dönüşümünü incelemek istersek;\n",
    "print(f\"Asıl veri:\\n{dfOriginal['review'][0]}\\n\\nTemizlenmiş yorum:\\n{\" \".join(dfData['review'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "72a07e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'deactivate' kelîmesnin gövdesi: deactiv\n",
      "'unsuccessful' kelîmesnin gövdesi: unsuccess\n"
     ]
    }
   ],
   "source": [
    "# Veri gerçekten kısalmış; fakat istemediğimiz bir şey 'reviewers' gibi hem yapım, hem çekim eki almış kelîmelerden\n",
    "# yapım eki de çıkarılmış; bunu yapmadığımız zamân model başarısı daha iyi olabilir;\n",
    "# Eğer iyi bir sonuç alamazsak, metîn gövdeleme işlemini yapmadan deneyebiliriz.\n",
    "# Bâzı durumlarda kelîmenin kökünü almak yerine gövdesini almak daha iyi bir çözüm olabilir,\n",
    "# Veriyi kelîme köklerini almadan evvel kaydettiğimiz için kelîme kökleme yerine kelîme gövdeleme işlemi uygulanıp,\n",
    "# performans karşılaştırması yapılabilir.\n",
    "# Bu arada PorterStemmer, olumsuzlukla ilgili yapım eklerini çıkartmıyor:\n",
    "print(f\"'deactivate' kelîmesnin gövdesi: {prStem.stem('deactivate')}\")\n",
    "print(f\"'unsuccessful' kelîmesnin gövdesi: {prStem.stem('unsuccessful')}\")\n",
    "# Devâm etmeden evvel köklenmiş veriyi ayrı dosyada saklamak isteyebiliriz:\n",
    "dfData.to_json('cleaned_and_stemmed_IMDB.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6a465708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Şimdi kelîmeleri birleştirip;\n",
    "dfData['review'] = [' '.join(sent) for sent in dfData['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dec08327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vektör hâline getirmek gerekiyor:\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words = 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "02cfc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verileri sayı dizisi şekline getirmek için;\n",
    "tokenizer.fit_on_texts(dfData['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "06f543b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData['sequences'] = tokenizer.texts_to_sequences(dfData['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "75c5a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metînleri eşit uzunluğa getirmek gerekiyor.\n",
    "maxLength = 100\n",
    "dfData['readySeq'] = list(pad_sequences(dfData['sequences'], maxlen = maxLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1592c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini test ve eğitim olarak ikiye ayırmadan evvel, y değerlerini sayısala çevirmeliyiz:\n",
    "dfData['readySentiment'] = dfData['sentiment'].apply(lambda x: nm.uint8(1) if x == 'positive' else nm.uint8(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "03857344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verileri ayırmamız lazım:\n",
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(dfData['readySeq'], dfData['readySentiment'],test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "ea9c7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verinin iç boyut biçimi şu an doğru görünmediği için önce veri düzleştirilmeli\n",
    "xTrain = nm.array([i.flatten() for i in xTrain])\n",
    "xTest = nm.array([i.flatten() for i in xTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "4b321540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiket verileri numpy dizisi hâline getirilmeli\n",
    "yTrain = yTrain.to_numpy()\n",
    "yTest = yTest.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "2cb3d9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,250,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">23,424</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │       \u001b[38;5;34m2,250,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m23,424\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │           \u001b[38;5;34m3,136\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m544\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,277,137</span> (8.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,277,137\u001b[0m (8.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,277,137</span> (8.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,277,137\u001b[0m (8.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Yeni model oluşturmalı ve bu sefer Dropout katmanı da eklemeliyiz:\n",
    "# Bu tür durumlar için modeli Sequential API yerine fonksiyonel API yöntemiyle oluşturmak daha iyidir.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(100,)))\n",
    "model.add(Embedding(\n",
    "    input_dim = 15000, output_dim = 150, input_length = maxLen\n",
    "    ))\n",
    "model.add(LSTM(32, activation = 'relu', return_sequences = True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(16, activation = 'relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "callEarlyStopping = EarlyStopping(monitor = 'val_loss', patience = 3,\n",
    "                                  restore_best_weights = True)\n",
    "\n",
    "filePath = \"C:\\AIMLProjects\\Keras\\imdbSentimentAnalysis\\model_v2.h5\"\n",
    "callModelCheckpoint = ModelCheckpoint(filePath, save_best_only = True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "1f3b656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.6744 - loss: 0.6297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 562ms/step - accuracy: 0.6751 - loss: 0.6288 - val_accuracy: 0.8484 - val_loss: 0.3570\n",
      "Epoch 2/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - accuracy: 0.8804 - loss: 0.4073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 538ms/step - accuracy: 0.8804 - loss: 0.4071 - val_accuracy: 0.8531 - val_loss: 0.3535\n",
      "Epoch 3/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 538ms/step - accuracy: 0.9105 - loss: 329.0284 - val_accuracy: 0.8243 - val_loss: 0.4276\n",
      "Epoch 4/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 532ms/step - accuracy: 0.8640 - loss: 0.3692 - val_accuracy: 0.8201 - val_loss: 0.4083\n",
      "Epoch 5/15\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 533ms/step - accuracy: 0.8968 - loss: 0.2870 - val_accuracy: 0.8249 - val_loss: 0.4139\n",
      "Doğruluk : 0.8934285640716553\n",
      "Kayıp : 0.2831476628780365\n",
      "Doğrulama seti doğruluğu : 0.8248571157455444\n",
      "Doğrulama kaybı : 0.4138679504394531\n"
     ]
    }
   ],
   "source": [
    "# Modelin derlenmesi ve eğitim:\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x = xTrain, y = yTrain, epochs = 15, batch_size = 256,\n",
    "          validation_split = 0.2,\n",
    "          callbacks = [callEarlyStopping, callModelCheckpoint]\n",
    "          )\n",
    "\n",
    "\n",
    "res = model.history.history\n",
    "\n",
    "print(f\"Doğruluk : {res['accuracy'][-1]}\")# \n",
    "print(f\"Kayıp : {res['loss'][-1]}\")# \n",
    "print(f\"Doğrulama seti doğruluğu : {res['val_accuracy'][-1]}\")# \n",
    "print(f\"Doğrulama kaybı : {res['val_loss'][-1]}\")# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "32e1ddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 173ms/step - accuracy: 0.8621 - loss: 0.3465\n"
     ]
    }
   ],
   "source": [
    "# Oldukça zorlu bir veri setiydi. Bu veri setinin üstesinden gelmek için şunlar yapılabilir:\n",
    "# - alınan kelîme sayısını biraz daha arttırmak,\n",
    "# - durma kelîmelerinin kapsamını genişletmek\n",
    "# - Modele bir iki katman daha eklenebilir (berâberinde Dropout katmanıyla berâber)\n",
    "\n",
    "# Şu anki hâliyle öncekinden çok daha başarılı bir sistem üretilmiş oldu;\n",
    "# sistemin biraz ezberlediği görünüyor;\n",
    "# fakat bundan emîn olmak için test veri seti üzerinde değerlendirme yapmak lazım:\n",
    "resOfTest = model.evaluate(xTest, yTest, batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f8be6",
   "metadata": {},
   "source": [
    "- Modelle ilgili durum hiç de fenâ değil; fakat bu kadar basit bir görev için az görünüyor..\n",
    "- Bunun önemli sebebinin kelîmeleri gövdelememiz olduğunu düşünüyorum;\n",
    "- Şimdi elimizdeki temizlenmiş, fakat gövdelenmemiş eğitim veri setiyle deneme yapmak iyi bir fikir olabilir.\n",
    "- Daha fazla bilgi için Sınıflandırma raporuna ('classification_report') bakılabilir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38843796",
   "metadata": {},
   "source": [
    "- Yazar : Mehmet Akif SOLAK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
